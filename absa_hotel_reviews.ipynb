{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmritaNambiar/INFO202_ABSA_Hotel_Reviews/blob/main/absa_hotel_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0316b585"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "8SiWUio8zjG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f632d4a5"
      },
      "outputs": [],
      "source": [
        "# Download necessary NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define the path to the dataset\n",
        "dataset_path = '/content/drive/MyDrive/[INFO 202]Annotated_Hotel_Reviews.csv'\n",
        "\n",
        "# Load the dataset into a Pandas DataFrame\n",
        "df_hotel_reviews = pd.read_csv(dataset_path)\n",
        "\n",
        "# Calculate the number of tokens in the 'review_text' column\n",
        "df_hotel_reviews['review_token_count'] = df_hotel_reviews['review_text'].apply(lambda x: len(word_tokenize(str(x))))\n",
        "\n",
        "# Arrange the rows by the number of tokens in descending order\n",
        "df_hotel_reviews = df_hotel_reviews.sort_values(by='review_token_count', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame with the new token count and sorted order\n",
        "print(\"\\nFirst 5 rows of the DataFrame after sorting by review_token_count (descending):\")\n",
        "print(df_hotel_reviews.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe2b3c05"
      },
      "outputs": [],
      "source": [
        "# Randomly selecting 100 reviews\n",
        "dataset_directory = os.path.dirname(dataset_path)\n",
        "\n",
        "# Define the new file path for the randomly selected 100 rows\n",
        "output_file_path = os.path.join(dataset_directory, 'Hotel_Reviews_Sample_100_Rows.csv')\n",
        "\n",
        "# Randomly select 100 rows from the DataFrame\n",
        "df_sample = df_hotel_reviews.sample(n=100, random_state=42)\n",
        "\n",
        "# Save the sample DataFrame to a new CSV file\n",
        "df_sample.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Successfully saved 100 randomly selected rows to: {output_file_path}\")\n",
        "\n",
        "# Display the head of the sample DataFrame to confirm\n",
        "print(\"\\nFirst 5 rows of the sample DataFrame:\")\n",
        "print(df_sample.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah0syHZ06h6K"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# 1. Load annotated hotel reviews\n",
        "# ------------------------------\n",
        "# The CSV contains one row per review, with one column per aspect\n",
        "# (Location, Room, Cleanliness, etc.) annotated as -1/0/1 for sentiment.[file:2]\n",
        "# Get the directory of the original dataset path\n",
        "\n",
        "# Define the path to the dataset\n",
        "dataset_path = '/content/drive/MyDrive/[INFO 202]Annotated_Hotel_Reviews.csv'\n",
        "\n",
        "# Load the dataset into a Pandas DataFrame\n",
        "df_hotel_reviews = pd.read_csv(dataset_path)\n",
        "\n",
        "print(df_hotel_reviews.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1KZMHM68PBA"
      },
      "outputs": [],
      "source": [
        "# Fixed aspects provided in the prompt\n",
        "aspect_cols = [\n",
        "    \"Location\",\n",
        "    \"Room\",\n",
        "    \"Cleanliness\",\n",
        "    \"Service\",\n",
        "    \"Facilities\",\n",
        "    \"Food_and_beverage\",\n",
        "    \"Price\",\n",
        "    \"Safety\",\n",
        "]\n",
        "\n",
        "# Map numeric labels in the CSV to class IDs for the model:\n",
        "#   -1 -> 0 (negative), 0 -> 1 (neutral), 1 -> 2 (positive)\n",
        "label_map = {-1: 0, 0: 1, 1: 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xllwhsm8wTa"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# 2. Text preprocessing function (BERT-style friendly)\n",
        "# -----------------------------------\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Light, BERT-friendly preprocessing:\n",
        "    - Normalize unicode (NFKC)\n",
        "    - Lowercase (for uncased BERT)\n",
        "    - Remove control characters\n",
        "    - Normalize whitespace\n",
        "    - Slightly normalize repeated punctuation\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Normalize Unicode (e.g., fancy quotes, accents) to a consistent form\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "    # Lowercasing is standard for bert-base-uncased\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove control characters while keeping newlines if you want them\n",
        "    text = \"\".join(\n",
        "        ch\n",
        "        for ch in text\n",
        "        if ch == \"\\n\" or unicodedata.category(ch)[0] != \"C\"\n",
        "    )\n",
        "\n",
        "    # Collapse multiple whitespace characters into a single space\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # Reduce long runs of \"!!!\" or \"???\", but keep punctuation\n",
        "    text = re.sub(r\"([!?]){3,}\", r\"\\1\\1\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the review_text column from the annotated CSV.\n",
        "df_hotel_reviews[\"review_text_clean\"] = df_hotel_reviews[\"review_text\"].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrrmxLMP8-Wa"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# 3. Convert table to (review, aspect) pairs\n",
        "# -----------------------------------\n",
        "# We create one training example per (review, aspect) pair.\n",
        "# Input to BERT: (review_text_clean, aspect_name)\n",
        "# Output label: sentiment for that aspect (negative/neutral/positive).\n",
        "records = []\n",
        "\n",
        "for _, row in df_hotel_reviews.iterrows():\n",
        "    review = row[\"review_text_clean\"]\n",
        "    rating = float(row[\"rating\"])  # overall rating (optional, not used here)\n",
        "\n",
        "    for aspect in aspect_cols:\n",
        "        raw_label = row[aspect]\n",
        "\n",
        "        # Skip if label is missing\n",
        "        if pd.isna(raw_label):\n",
        "            continue\n",
        "\n",
        "        raw_label = int(raw_label)\n",
        "\n",
        "        # Skip unusual labels if any\n",
        "        if raw_label not in label_map:\n",
        "            continue\n",
        "\n",
        "        records.append(\n",
        "            {\n",
        "                \"review\": review,\n",
        "                # Use a nicer surface form for the aspect\n",
        "                \"aspect\": aspect.replace(\"_\", \" & \"),\n",
        "                # Map to -1/0/1 for model training\n",
        "                \"sent_class\": label_map[raw_label],\n",
        "                \"rating\": rating,\n",
        "            }\n",
        "        )\n",
        "\n",
        "# Create a DataFrame with one row per (review, aspect) pair\n",
        "data = pd.DataFrame(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk8fVRUQ9Ibq"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# 4. Train / validation split\n",
        "# -----------------------------------\n",
        "# Stratify by sentiment to keep label distribution similar in both splits.\n",
        "train_df, val_df = train_test_split(\n",
        "    data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=data[\"sent_class\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qqw3Gau9VCL"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# 5. Tokenizer (BERT) and Dataset\n",
        "# -----------------------------------\n",
        "# BERT tokenizer will:\n",
        "# - tokenize into subwords\n",
        "# - add [CLS], [SEP] tokens\n",
        "# - create token type ids for sentence pair tasks.[file:1]\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "class AspectSentimentDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for aspect-based sentiment:\n",
        "    each item = (input_ids, attention_mask, token_type_ids, label)\n",
        "    where inputs correspond to:\n",
        "        text = review text\n",
        "        text_pair = aspect phrase\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len=256):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = row[\"review\"]\n",
        "        aspect = row[\"aspect\"]\n",
        "        label = int(row[\"sent_class\"])\n",
        "\n",
        "        # Sentence-pair encoding: (review, aspect)\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            aspect,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Remove batch dimension for DataLoader\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "\n",
        "# Instantiate datasets\n",
        "train_ds = AspectSentimentDataset(train_df, tokenizer)\n",
        "val_ds = AspectSentimentDataset(val_df, tokenizer)\n",
        "\n",
        "# DataLoaders for mini-batch training\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHevMEZS9kff"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# 6. Model, optimizer, scheduler setup\n",
        "# -----------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Compute Class Weights (To handle imbalance)\n",
        "class_weights_arr = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.array([0, 1, 2]),\n",
        "    y=train_df['sent_class']\n",
        ")\n",
        "class_weights = torch.tensor(class_weights_arr, dtype=torch.float).to(device)\n",
        "\n",
        "print(\"Computed Class Weights:\")\n",
        "for i, label in enumerate(['negative', 'neutral', 'positive']):\n",
        "    print(f\"  {label}: {class_weights[i]:.4f}\")\n",
        "\n",
        "# BERT with a classification head\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=3,\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# AdamW is recommended for transformer models\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Number of epochs and training steps for scheduler\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "\n",
        "# Linear warmup + decay learning rate schedule\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * num_training_steps),\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag03VaDH9uow"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# 7. Training and validation loop\n",
        "# -----------------------------------\n",
        "import torch.nn as nn\n",
        "\n",
        "def run_epoch(dataloader, training=True):\n",
        "    \"\"\"\n",
        "    Run one epoch of training or validation.\n",
        "\n",
        "    Returns:\n",
        "    - average loss\n",
        "    - all predictions\n",
        "    - all true labels\n",
        "    \"\"\"\n",
        "    if training:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    # Use weighted CrossEntropyLoss\n",
        "    loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Move all batch tensors to device (CPU/GPU)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Enable gradient computation only during training\n",
        "        with torch.set_grad_enabled(training):\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            # Calculate loss with weights\n",
        "            loss = loss_fct(logits.view(-1, 3), batch[\"labels\"].view(-1))\n",
        "\n",
        "        if training:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Accumulate loss (weighted by batch size)\n",
        "        total_loss += loss.item() * batch[\"input_ids\"].size(0)\n",
        "\n",
        "        # Move predictions and labels back to CPU for metrics\n",
        "        preds = torch.argmax(logits, dim=-1).detach().cpu()\n",
        "        labels = batch[\"labels\"].detach().cpu()\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.tolist())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    return avg_loss, all_preds, all_labels\n",
        "\n",
        "\n",
        "# Train for a few epochs\n",
        "print(\"Starting training with class weights...\")\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, _, _ = run_epoch(train_loader, training=True)\n",
        "    val_loss, val_preds, val_labels = run_epoch(val_loader, training=False)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"  Train loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val loss:   {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tumu_Z2k9x3-"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# 8. Evaluation: classification metrics\n",
        "# -----------------------------------\n",
        "\n",
        "# Map class IDs sentiment labels\n",
        "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "print(\n",
        "    classification_report(\n",
        "        val_labels,\n",
        "        val_preds,\n",
        "        target_names=target_names,\n",
        "        digits=3,\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1be7fb5"
      },
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(val_labels, val_preds)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f721ee3"
      },
      "source": [
        "print(\"Class distribution in training data:\")\n",
        "class_counts = train_df['sent_class'].value_counts(normalize=True) * 100\n",
        "# Map back to human-readable labels for clarity\n",
        "class_counts = class_counts.rename(index={0: 'negative', 1: 'neutral', 2: 'positive'})\n",
        "print(class_counts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37768e1e"
      },
      "source": [
        "print(\"Class distribution in validation data:\")\n",
        "class_counts_val = val_df['sent_class'].value_counts(normalize=True) * 100\n",
        "# Map back to human-readable labels for clarity\n",
        "class_counts_val = class_counts_val.rename(index={0: 'negative', 1: 'neutral', 2: 'positive'})\n",
        "print(class_counts_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acAH73pa90Kk"
      },
      "outputs": [],
      "source": [
        "# Inference helper for decision support\n",
        "\n",
        "# Simple mapping from ID -> label\n",
        "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "\n",
        "def analyze_review(review_text: str):\n",
        "    \"\"\"\n",
        "    Run aspect-based sentiment analysis on a single review.\n",
        "\n",
        "    Steps:\n",
        "    - Apply the same preprocessing as training.\n",
        "    - For each fixed aspect, feed (review, aspect) into BERT.\n",
        "    - Return a dict: {aspect_name: sentiment_label}.\n",
        "\n",
        "    This can be used in a UI to show per-aspect pros/cons\n",
        "    for decision making and transparency.\n",
        "    \"\"\"\n",
        "    clean = clean_text(review_text)\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    for aspect in aspect_cols:\n",
        "        aspect_name = aspect.replace(\"_\", \" & \")\n",
        "\n",
        "        # Encode (review, aspect) as a BERT sentence pair\n",
        "        enc = tokenizer(\n",
        "            clean,\n",
        "            aspect_name,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        # Predict sentiment\n",
        "        with torch.no_grad():\n",
        "            logits = model(**enc).logits\n",
        "        pred_id = int(torch.argmax(logits, dim=-1).cpu().item())\n",
        "\n",
        "        results[aspect_name] = id2label[pred_id]\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Check if model and tokenizer exist in memory\n",
        "if 'model' not in globals() or 'tokenizer' not in globals():\n",
        "    print(\"\\u26a0\\ufe0f Error: Model or Tokenizer not found!\")\n",
        "    print(\"Please re-run the 'Model Setup' and 'Training' cells to define and train the model before saving.\")\n",
        "else:\n",
        "    # Save the model and tokenizer to a local directory\n",
        "    output_dir = \"hotel_absa_model\"\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    print(f\"Model and tokenizer saved to directory: {output_dir}\")\n",
        "\n",
        "    # Create a zip archive of the model directory for easier download\n",
        "    shutil.make_archive(output_dir, 'zip', output_dir)\n",
        "    print(f\"Created zip archive: {output_dir}.zip\")\n",
        "\n",
        "    # To download the file using code (optional):\n",
        "    # from google.colab import files\n",
        "    # files.download(f\"{output_dir}.zip\")"
      ],
      "metadata": {
        "id": "XGMd2mPD8iJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "example_review = \"\"\"\n",
        "The location was perfect and the room was very clean,\n",
        "but the staff were rude and the breakfast was disappointing.\n",
        "\"\"\"\n",
        "\n",
        "aspect_sentiments = analyze_review(example_review)\n",
        "print(aspect_sentiments)\n",
        "# Example output (depends on training):\n",
        "# {\n",
        "#   'Location': 'positive',\n",
        "#   'Room': 'positive',\n",
        "#   'Cleanliness': 'positive',\n",
        "#   'Service': 'negative',\n",
        "#   'Facilities': 'neutral',\n",
        "#   'Food & beverage': 'negative',\n",
        "#   'Price': 'neutral',\n",
        "#   'Safety': 'neutral'\n",
        "# }"
      ],
      "metadata": {
        "id": "BDxLa9JGZSqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}